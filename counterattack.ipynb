{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24dbd422-952b-4d3d-b657-ead51d4e717c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## _A Graph Neural Network deep-dive into successful counterattacks_\n",
    "\n",
    "### 1. Installing the required Python libraries:\n",
    "  Make sure the requirement.txt file is in the directory. Navigate to the directory using command prompt and run\n",
    " ```\n",
    " pip install -r requirements.txt\n",
    " ```\n",
    " or for MacOS\n",
    " ```\n",
    " pip install -r requirements_macos.txt\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29fa6f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import Checkbox, Dropdown, Accordion, VBox\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.calibration import calibration_curve\n",
    "from spektral.data import Dataset, Graph, DisjointLoader\n",
    "from spektral.layers import CrystalConv, GlobalAvgPool\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import AUC\n",
    "\n",
    "import copy\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import requests\n",
    "import progressbar\n",
    "from os.path import isfile\n",
    "import time\n",
    "\n",
    "# Setting up the logger\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "logger.addHandler(stdout_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45961bb5-8f7f-4d9a-8451-279aefc39d07",
   "metadata": {},
   "source": [
    "### 2. Data Processing\n",
    "\n",
    "- `get_data()` fetches the specified file (Women, Men, Combined) from a public s3 bucket. The data is stored locally in {women, men, combined}.pkl. It is automatically loaded after downloading.\n",
    "- We need to convert that data into a Spektral specific `Dataset` class to include node features, edge features and the adjacency matrix. We do this with the `CounterDataset` class that inherits from the `Dataset` class. \n",
    "- The `CounterDataset` class returns a list of `Graph` objects.\n",
    "\n",
    "To help process the data we create a couple interactive checkboxes in 2.2 - 2.6 to help select node features, edge features etc.\n",
    "\n",
    "##### !! For more information on GNN data structures etc. Check out [Spektral's Getting Started](https://graphneural.network/getting-started/) page !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55f0b103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file_name):\n",
    "    '''\n",
    "    Fetches the file from the location, loads it into memory and returns the data.\n",
    "    '''\n",
    "    if not isfile(file_name):\n",
    "        url = f\"https://ussf-ssac-23-soccer-gnn.s3.us-east-2.amazonaws.com/public/counterattack/{file_name}\"\n",
    "\n",
    "        logger.info(f\"Downloading data from {url}...\")\n",
    "        \n",
    "        r = requests.get(url, stream=True)\n",
    "        # Fancy code to print progress bar\n",
    "        block_size = 1024\n",
    "        n_chunk = 1000\n",
    "        file_size = int(r.headers.get('Content-Length', None))\n",
    "        num_bars = np.ceil(file_size / (n_chunk * block_size))\n",
    "        bar =  progressbar.ProgressBar(maxval=num_bars).start()\n",
    "        with open(file_name, 'wb') as f:\n",
    "            for i, chunk in enumerate(r.iter_content(chunk_size=n_chunk * block_size)):\n",
    "                f.write(chunk)\n",
    "                bar.update(i+1)\n",
    "                # Add a little sleep so you can see the bar progress\n",
    "                time.sleep(0.05)\n",
    "\n",
    "        logger.info(\"File downloaded successfully!\")\n",
    "        \n",
    "    logger.info(f\"Opening {file_name}...\")\n",
    "    with open(file_name, 'rb') as handle:\n",
    "        data = pickle.load(handle)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81266f4c-59e7-4002-bbd2-a5ea1f95bebc",
   "metadata": {},
   "source": [
    "### 2.1 Choose Dataset Type\n",
    "\n",
    "Choose the type of file for training the GNN. Options available are women's data (which include women's data from the 2022 NWSL and International women's soccer between 2020 and 2022), men's data (data from the 2022 MLS) and a combined data file (includes both women's and men's data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "140c1cf4-e49e-4dcc-b8e0-f043d2da392b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose File for training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc9906e0cf24cc7901abe49d23cb8cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(options=('Women', 'Men', 'Combined'), value='Women')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Choose File for training:\")\n",
    "file_widget = Dropdown(\n",
    "    options=['Women', 'Men', 'Combined'],\n",
    "    value='Women',\n",
    "    disabled=False,\n",
    ")\n",
    "display(file_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e511a5e",
   "metadata": {},
   "source": [
    "### 2.2 (Down)load Data\n",
    "Using the file selection widget we load the appropriate file from s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13b29edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening women.pkl...\n"
     ]
    }
   ],
   "source": [
    "file_name = file_widget.value.lower() + '.pkl'\n",
    "# Obtain the data\n",
    "og_data = get_data(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc51426-a03b-443d-8869-5d337b7c05e3",
   "metadata": {},
   "source": [
    "### 2.2 Choose Adjacency Matrix\n",
    "\n",
    "Available options:\n",
    "\n",
    "- **normal** - connects attacking players amongst themselves, defensive players amongst themselves and the attacking and defending players are connected through the ball.\n",
    "- **delaunay** - connects a few attacking players and defending players in a delaunay matrix fashion. Exact layout depends on player positioning during the frame in question.\n",
    "- **dense** - connects all the players and the ball to each other\n",
    "- **dense_ap** - connects all the attacking players to each other and to the defensive players.\n",
    "- **dense_dp** - connects all the defending players to each other and to the attacking players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eab8f50-2704-4900-8137-d5aa8ca30059",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Choose Adjacency Matrix:\")\n",
    "adj_matrix = Dropdown(\n",
    "    options=['normal', 'delaunay', 'dense', 'dense_ap', 'dense_dp'],\n",
    "    value='normal',\n",
    "    disabled=False,\n",
    ")\n",
    "display(adj_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab384df4-477f-404e-875b-322da345cee9",
   "metadata": {},
   "source": [
    "### 2.3 Choose Edge Features\n",
    "\n",
    "Available options:\n",
    "\n",
    "- **Player Distance** - Distance between two players connected to each other\n",
    "- **Speed Difference** - Speed difference between two players connected to each other\n",
    "- **Positional Sine angle** - Sine of the angle created between two players in the edge\n",
    "- **Positional Cosine angle** - Cosine of the angle created between two players in the edge\n",
    "- **Velocity Sine angle** - Sine of the angle created between the velocity vectors of two players in the edge\n",
    "- **Velocity Cosine angle** - Coine of the angle created between the velocity vectors of two players in the edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004a8a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_dist = Checkbox(\n",
    "    value=True,\n",
    "    description='Player Distance',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "speed_diff_matrix = Checkbox(\n",
    "    value=True,\n",
    "    description='Speed Difference',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "pos_sin_angle = Checkbox(\n",
    "    value=True,\n",
    "    description='Positional Sine angle',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "pos_cos_angle = Checkbox(\n",
    "    value=True,\n",
    "    description='Positional Cosine angle',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "vel_sin_angle = Checkbox(\n",
    "    value=True,\n",
    "    description='Velocity Sine angle',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "vel_cos_angle = Checkbox(\n",
    "    value=True,\n",
    "    description='Velocity Cosine angle',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Add the user selection of edge features in edge_f_box list.\n",
    "edge_f_box = VBox([player_dist, speed_diff_matrix, pos_sin_angle, pos_cos_angle, vel_sin_angle, vel_cos_angle])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107c0a56",
   "metadata": {},
   "source": [
    "### 2.4 Choose Node Features\n",
    "\n",
    "##### Node Features description\n",
    "- **x coordinate** - x coordinate on the 2D pitch for the player\n",
    "- **y coordinate** - y coordinate on the 2D pitch for the player\n",
    "- **vx** - Velocity vector's x coordinate\n",
    "- **vy** - Velocity vector's y coordinate\n",
    "- **Velocity** - magnitude of the velocity\n",
    "- **Velocity Angle** - angle made by the velocity vector\n",
    "- **Distance to Goal** - distance of the player from the goal post\n",
    "- **Angle with Goal** - angle made by the player with the goal\n",
    "- **Distance to Ball** - distance from the ball (always 0 for the ball)\n",
    "- **Angle with Ball** - angle made with the ball (always 0 for the ball)\n",
    "- **Attacking Team Flag** - 1 if the team is attacking, 0 if not and for the ball\n",
    "- **Potential Receiver** - 1 if player is a potential receiver, 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803e4394-862e-41e1-84aa-aaf9987f5277",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_coord = Checkbox(\n",
    "    value=True,\n",
    "    description='x coordinate',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "y_coord = Checkbox(\n",
    "    value=True,\n",
    "    description='y coordinate',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "vx = Checkbox(\n",
    "    value=True,\n",
    "    description='vX',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "\n",
    "vy = Checkbox(\n",
    "    value=True,\n",
    "    description='vY',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "\n",
    "v = Checkbox(\n",
    "    value=True,\n",
    "    description='Velocity',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "\n",
    "velocity_angle = Checkbox(\n",
    "    value=True,\n",
    "    description='Velocity Angle',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "dist_goal = Checkbox(\n",
    "    value=True,\n",
    "    description='Distance to Goal',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "goal_angle = Checkbox(\n",
    "    value=True,\n",
    "    description='Angle with Goal',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "\n",
    "dist_ball = Checkbox(\n",
    "    value=True,\n",
    "    description='Distance to Ball',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "ball_angle = Checkbox(\n",
    "    value=True,\n",
    "    description='Angle with Ball',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "\n",
    "is_attacking = Checkbox(\n",
    "    value=True,\n",
    "    description='Attacking Team Flag',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "potential_receiver = Checkbox(\n",
    "    value=True,\n",
    "    description='Potential Receiver',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Add the user selection of node features in node_f_box list.\n",
    "node_f_box = VBox([x_coord, y_coord, vx, vy, v, velocity_angle, dist_goal, goal_angle, dist_ball, ball_angle, \n",
    "                is_attacking, potential_receiver])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c0104d",
   "metadata": {},
   "source": [
    "### 2.5 Display Checkboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7481c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "accordion = Accordion(children=[edge_f_box, node_f_box])\n",
    "accordion.set_title(0, 'Edge Features')\n",
    "accordion.set_title(1, 'Node Features')\n",
    "accordion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b48ee9-3aa5-407b-9b7c-f38c9616368f",
   "metadata": {},
   "source": [
    "### 2.6 Update dataset with selected features\n",
    "The loaded dataset is updated to include only the selected node features and edge features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b8d2bb-6f02-474a-b4b9-c602e2317ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_features(data, gender=None):\n",
    "    \"\"\"\n",
    "    gender: either None, 'm' or 'w'. Use m/w when trying out gender-aware model, this will add an extra node_feature\n",
    "    the node feature will be 0 (m) and 1 (f). This is only relevent after loading both the M and W datasets (not implemented atm)\n",
    "    \"\"\"\n",
    "    edge_feature_idxs = [idx for idx, x in enumerate(edge_f_box.children) if x.value]\n",
    "    node_feature_idxs = [idx for idx, x in enumerate(node_f_box.children) if x.value]\n",
    "    node_features = [x.description for x in node_f_box.children if x.value]\n",
    "\n",
    "    # Check for empty edge features or node features.\n",
    "    if not any(edge_feature_idxs) and not any(node_feature_idxs):\n",
    "        print(\"\\nCannot have zero edge features and zero node features.\\n\")\n",
    "        print(\"\\nDefaulting to the previous configuration.\")    \n",
    "    else:    \n",
    "        if not gender:\n",
    "            data[adj_matrix.value]['normal_e'] = [x[:, edge_feature_idxs] for x in data[adj_matrix.value]['normal_e']]\n",
    "            data[adj_matrix.value]['normal_x'] = [x[:, node_feature_idxs] for x in data[adj_matrix.value]['normal_x']]\n",
    "        elif gender == 'm':\n",
    "            data[adj_matrix.value]['normal_e'] = [x[:, edge_feature_idxs] for x in data[adj_matrix.value]['normal_e']]\n",
    "            data[adj_matrix.value]['normal_x'] = [np.append(\n",
    "                x[:, node_feature_idxs], \n",
    "                np.zeros(shape=(x.shape[0], 1)), axis=1\n",
    "            ) for x in data[adj_matrix.value]['normal_x']]\n",
    "        elif gender == 'w':\n",
    "            data[adj_matrix.value]['normal_e'] = [x[:, edge_feature_idxs] for x in data[adj_matrix.value]['normal_e']]\n",
    "            data[adj_matrix.value]['normal_x'] = [np.append(\n",
    "                x[:, node_feature_idxs], \n",
    "                np.ones(shape=(x.shape[0], 1)), axis=1\n",
    "            ) for x in data[adj_matrix.value]['normal_x']]\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = filter_features(og_data.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402b04b5",
   "metadata": {},
   "source": [
    "### 2.7 Convert to Spektral Graphs\n",
    "Finally, we convert the data to Spektrals `Graph` representation. We also select the adjecancy matrix type.\n",
    "\n",
    "##### !! [More information on creating custom datasets with Spektal](https://graphneural.network/creating-dataset/) !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a32e3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CounterDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Convert raw Graph data to a CounterDataset with Dataset class from the spektral library to include node features, \n",
    "    edge features and the adjacency matrix.\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        '''\n",
    "        Constructor to load parameters.\n",
    "        '''\n",
    "        self.data = kwargs['data']\n",
    "        self.matrix_type = kwargs['matrix_type']\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def read(self):\n",
    "        '''\n",
    "        Overriding the read function - to return a list of Graph objects\n",
    "        '''\n",
    "        logger.info(\"Loading Pass Dataset.\")\n",
    "        \n",
    "        data = self.data\n",
    "        # Choosing the data with the required matrix type.\n",
    "        data_mat = data[self.matrix_type]\n",
    "        \n",
    "        # Print Graph information\n",
    "        logger.info(\n",
    "            f\"\"\"node_features (x): {data_mat['x'][0].shape}\n",
    "             \\n adj_matrix (a): {data_mat['a'][0].shape}\n",
    "             \\n edge_features (e): {data_mat['e'][0].shape}\n",
    "             \"\"\"\n",
    "        )\n",
    "        \n",
    "        # Return a list of Graph objects\n",
    "        return [\n",
    "            Graph(x=x, a=a, e=e, y=y) for x, a, e, y in zip(\n",
    "                data_mat['x'], data_mat['a'], data_mat['e'], data['binary']\n",
    "            )\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d45b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset in the CounterDataset format.\n",
    "dataset = CounterDataset(data = data, matrix_type = adj_matrix.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a787da2-6592-4445-8e16-5eba2ed869a9",
   "metadata": {},
   "source": [
    "### 3 Training Graph Neural Network\n",
    "\n",
    "### 3.1 Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861944d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3  # Learning rate\n",
    "epochs = 150  # Number of training epochs\n",
    "batch_size = 16  # Batch size\n",
    "channels = 128  # Hidden units for the neural network\n",
    "layers = 3  # Number of CrystalConv layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48c3dc9",
   "metadata": {},
   "source": [
    "### 3.2 Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8224411",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = max(g.n_nodes for g in dataset) # Number of nodes\n",
    "F = dataset.n_node_features  # Dimension of node features\n",
    "S = dataset.n_edge_features  # Dimension of edge features\n",
    "n_out = dataset.n_labels  # Dimension of the target\n",
    "n = len(dataset) # Number of samples in the dataset\n",
    "\n",
    "# Train/test split for the dataset\n",
    "idxs = np.random.RandomState(seed=15).permutation(len(dataset))\n",
    "split_va, split_te = int(0.7 * len(dataset)), int(0.69 * len(dataset))\n",
    "idx_tr, idx_va, idx_te = np.split(idxs, [split_va, split_te])\n",
    "dataset_tr = dataset[idx_tr]\n",
    "dataset_te = dataset[idx_te]\n",
    "loader_tr = DisjointLoader(dataset_tr, batch_size=batch_size, epochs=epochs)\n",
    "loader_te = DisjointLoader(dataset_te, batch_size=batch_size, epochs=1, shuffle = False)\n",
    "\n",
    "# Display dataset information on target variable.\n",
    "logger.info(f\"n: {n}\")\n",
    "logger.info(f\"Pct successful total: {round(np.asarray([graph.y[0] for graph in dataset]).sum() / n, 2)}\")\n",
    "logger.info(f\"Pct successful train: {round(np.asarray([graph.y[0] for graph in dataset_tr]).sum() / (n * .7), 2)}\")\n",
    "logger.info(f\"Pct successful test: {round(np.asarray([graph.y[0] for graph in dataset_te]).sum() / (n * .3), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d780a4d",
   "metadata": {},
   "source": [
    "### 3.3 Build GNN Model\n",
    "Build the GNN model using the Spektral preferred structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0473cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(Model):\n",
    "    '''\n",
    "    Building the Graph Neural Network configuration with Model as the parent class \n",
    "    from spektral library.\n",
    "    '''\n",
    "    def __init__(self, n_layers):\n",
    "        '''\n",
    "        Constructor code for setting up the layers needed for training the model.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.conv1 = CrystalConv()\n",
    "        self.convs = []\n",
    "        for _ in range(1, n_layers):\n",
    "            self.convs.append(\n",
    "                CrystalConv()\n",
    "            )\n",
    "        self.pool = GlobalAvgPool()\n",
    "        self.dense1 = Dense(channels, activation=\"relu\")\n",
    "        self.dropout = Dropout(0.5)\n",
    "        self.dense2 = Dense(channels, activation=\"relu\")\n",
    "        self.dense3 = Dense(n_out, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        '''\n",
    "        Build the neural network.\n",
    "        '''\n",
    "        x, a, e, i = inputs\n",
    "        x = self.conv1([x, a, e])\n",
    "        for conv in self.convs:\n",
    "            x = conv([x, a, e])\n",
    "        x = self.pool([x, i])\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.dense3(x)\n",
    "\n",
    "\n",
    "# Build model\n",
    "model = GNN(layers)\n",
    "# Setup the optimizer\n",
    "optimizer = Adam(learning_rate)\n",
    "# Set up the logloss function\n",
    "loss_fn = BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6deffde8",
   "metadata": {},
   "source": [
    "### 3.4 Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28b09eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(input_signature=loader_tr.tf_signature(), experimental_relax_shapes=True)\n",
    "def train_step(inputs, target):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        loss = loss_fn(target, predictions) + sum(model.losses)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Print loss at each step of training.\n",
    "step = loss = 0\n",
    "for batch in loader_tr:\n",
    "    step += 1\n",
    "    loss += train_step(*batch)\n",
    "    if step == loader_tr.steps_per_epoch:\n",
    "        step = 0\n",
    "        print(\"Loss: {}\".format(loss / loader_tr.steps_per_epoch))\n",
    "        loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dcc639-9591-4d05-bb73-bb5ed6f5fcf7",
   "metadata": {},
   "source": [
    "### 4 Evaluate Model Performance\n",
    "\n",
    "### 4.1 Evaluate ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d43864",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Testing Model...\")\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "# Add the true values and the predicted values in the list\n",
    "for batch in loader_te:\n",
    "    inputs, target = batch\n",
    "    p = model(inputs, training=False)\n",
    "    y_true.append(target)\n",
    "    y_pred.append(p.numpy())\n",
    "\n",
    "# Calculate the ROC-AUC metric\n",
    "y_true = np.vstack(y_true)\n",
    "y_pred = np.vstack(y_pred)\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_true, y_pred)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "# Plot the ROC w/ True positive rate on the y-axis and False positive rate on the x-axis.\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3136873-57bf-4b0a-bd99-032a1924fa5b",
   "metadata": {},
   "source": [
    "### 4.2 Evaluate Calibration\n",
    "- Calculate Expected Calibration Error\n",
    "- Plot Calibration Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfa11b4-0a69-4e58-8cd6-bdd049540c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the dataset in the CounterDataset format.\n",
    "dataset_c = CounterDataset(data=data, matrix_type = adj_matrix.value)\n",
    "\n",
    "# Setup the loader.\n",
    "loader = DisjointLoader(dataset_c, batch_size=1, epochs=1, shuffle = False)\n",
    "\n",
    "# Set up an empty pandas dataframe.\n",
    "ece_df = pd.DataFrame(columns = ['output_pp', 'predicted', 'target', 'result'])\n",
    "\n",
    "# Compute the predictions and save them in the Pandas DataFrame.\n",
    "for batch in loader:\n",
    "    inputs, target = batch\n",
    "    p = model(inputs, training=False)\n",
    "    original_prediction = p.numpy()[0][0]\n",
    "    \n",
    "    # Threshold set to 0.5\n",
    "    predicted_value = 1 if original_prediction >= 0.5 else 0 \n",
    "    ece_df.loc[len(ece_df)] = [original_prediction, predicted_value, target[0][0], 1 if predicted_value == target[0][0] else 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d87aba-0718-48dd-affa-a231b7d46f97",
   "metadata": {},
   "source": [
    "##### Expected Calibration Error (ECE)\n",
    "\n",
    "$$ ECE = \\sum_{k = 1}^{K} \\frac{|B_k|}{N}  \\left| \\left( \\frac{1}{|B_k|} \\sum_{i \\in B_k} y_i \\right) - \\left( \\frac{1}{|B_k|} \\sum_{i \\in B_k} \\hat{y_i} \\right) \\right| $$\n",
    "\n",
    "We distribute the outcome into K bins, and compute the difference between the average prediction in each bin and the average expected outcome for the examples in each bin. $B_k$ corresponds to the set of examples in the $k$-th bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00927f4-f8d5-4510-bac9-f19ad585a6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the bins\n",
    "bin_ranges = [(0, 0.1), (0.1, 0.2), (0.2, 0.3), (0.3, 0.4), (0.4, 0.5), (0.5, 0.6), (0.6, 0.7), (0.7, 0.8), (0.8, 0.9), (0.9, 1.0)]\n",
    "bin_calc = pd.DataFrame(columns = ['bin', 'count', 'accuracy', 'avg_pp', 'acc-conf', 'count_into_acc-conf'])\n",
    "\n",
    "for i, bin_range in enumerate(bin_ranges):\n",
    "    # Get the higher and lower end of the bins\n",
    "    lower, higher = bin_range[0], bin_range[1]\n",
    "    \n",
    "    # Get the probability outputs within the range\n",
    "    bin_calc_temp = ece_df.loc[(ece_df['output_pp'] > lower) & (ece_df['output_pp'] <= higher)]\n",
    "    count = bin_calc_temp.shape[0]\n",
    "    \n",
    "    # Compute parameters needed to calculate ECE \n",
    "    if count > 0:\n",
    "        total_corrects = bin_calc_temp[(bin_calc_temp['result'] == 1)].shape[0]\n",
    "        accuracy = total_corrects / count\n",
    "        avg_pp = bin_calc_temp['output_pp'].mean()\n",
    "        acc_conf = abs(accuracy - avg_pp)\n",
    "\n",
    "        bin_calc.loc[i] = [bin_range, count, accuracy, avg_pp, acc_conf, count*acc_conf]\n",
    "        \n",
    "# Print ECE value    \n",
    "print(\"ECE is : \" + str(bin_calc['count_into_acc-conf'].sum() / bin_calc['count'].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0ec56d",
   "metadata": {},
   "source": [
    "##### Calibration Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f312ed54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the sklearn calibration_curve() function to obtain calibration values for the model.\n",
    "cal_y, cal_x = calibration_curve(ece_df['target'], ece_df['output_pp'], n_bins = 10)\n",
    "\n",
    "# Plot the calibration curve.\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(cal_x, cal_y, marker = '.')\n",
    "plt.plot([0, 1], [0, 1], ls = '--', color = 'green', label = 'Ideal calibration')\n",
    "leg = plt.legend(loc = 'upper left')\n",
    "plt.xlabel('Average Predicted Probability in each bin')\n",
    "plt.ylabel('Ratio of positives')\n",
    "plt.title(\"Calibration Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48510897-d344-4bcf-8c3f-e3080a8ae3c0",
   "metadata": {},
   "source": [
    "### 5. Measure Feature Importance\n",
    "\n",
    "[Permutation Feature Importance](https://christophm.github.io/interpretable-ml-book/feature-importance.html) ([Altmann & Tolosi, 2010](https://www.researchgate.net/publication/43130914_Permutation_importance_A_corrected_feature_importance_measure)) allows us to identify the importance of each individual feature by measuring the increase in prediction error when breaking the relationship between individual features and the observed result through the application of a random permutation to the featureâ€™s values. In other words, we use the test set to randomly shuffle the values for one feature and recalculate the model error.\n",
    "\n",
    "To do this we create a new dataset were the feature of either the attacking team or the defending team are shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece4f2b5-6047-4538-8dd7-e33bcf25d1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShuffledCounterDataset(Dataset):\n",
    "    '''\n",
    "    Convert raw Graph data to a ShuffledCounterDataset with Dataset class from the spektral library to include node features, \n",
    "    edge features and the adjacency matrix.\n",
    "    '''\n",
    "    def __init__(self, **kwargs):\n",
    "        '''\n",
    "        Constructor to load the parameters.\n",
    "        '''\n",
    "        self.data = kwargs['data']\n",
    "        self.node_feature_shuffle = kwargs['node_feature_shuffle']\n",
    "        self.player_type = kwargs['player_type']\n",
    "        self.matrix_type = kwargs['matrix_type']\n",
    "        self.flag_index = kwargs['flag_index']\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def read(self):\n",
    "        '''\n",
    "        Overriding the read function - to return a list of Graph objects.\n",
    "        Permuting the features in this function.\n",
    "        '''\n",
    "        data = copy.deepcopy(self.data)\n",
    "        data_mat = data[self.matrix_type]\n",
    "        \n",
    "        # Check the type of players to be shuffled\n",
    "        if self.player_type == 'attacking':\n",
    "            for i in range(len(data_mat['x'])):\n",
    "                arr = data_mat['x'][i]\n",
    "                  \n",
    "                # Get the appropriate type of player\n",
    "                atts = arr[0:-1][arr[:-1, self.flag_index] == 1].copy()\n",
    "                defs = arr[0:-1][arr[:-1, self.flag_index] == 0].copy()\n",
    "                ball = arr[-1].copy()\n",
    "\n",
    "                # Shuffle the feature requested\n",
    "                random.shuffle(atts[:, self.node_feature_shuffle])\n",
    "\n",
    "                # Assign back the shuffled values to the correct place they came from\n",
    "                arr[0:-1][arr[:-1, self.flag_index] == 1] = atts\n",
    "                arr[0:-1][arr[:-1, self.flag_index] == 0] = defs\n",
    "           \n",
    "        else:\n",
    "            for i in range(len(data_mat['x'])):\n",
    "                arr = data_mat['x'][i]\n",
    "                    \n",
    "                # Get the appropriate type of player\n",
    "                atts = arr[0:-1][arr[:-1, self.flag_index] == 1].copy()\n",
    "                defs = arr[0:-1][arr[:-1, self.flag_index] == 0].copy()\n",
    "                ball = arr[-1].copy()\n",
    "\n",
    "                # Shuffle the feature requested\n",
    "                random.shuffle(defs[:, self.node_feature_shuffle])\n",
    "\n",
    "                # Assign back the shuffled values to the correct place they came from\n",
    "                arr[0:-1][arr[:-1, self.flag_index] == 1] = atts\n",
    "                arr[0:-1][arr[:-1, self.flag_index] == 0] = defs\n",
    "        \n",
    "        \n",
    "        return [\n",
    "            Graph(x=x, a=a, e=e, y=y) for x, a, e, y in zip(data_mat['x'], data_mat['a'], data_mat['e'], data['binary'])\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f511ff-fbfa-429f-8939-fa6ab71d491c",
   "metadata": {},
   "source": [
    "#### Select Attack or Defense for Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fe9ae0-a3cc-4203-b9cb-54351ee69f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose feature importance\n",
    "print(\"Choose Player type for testing feature importance:\")\n",
    "player_type = Dropdown(\n",
    "    options=['Attacking', 'Defending'],\n",
    "    value='Attacking',\n",
    "    disabled=False,\n",
    ")\n",
    "display(player_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a5b6fd-fb55-4906-8446-3998afcf5c5f",
   "metadata": {},
   "source": [
    "#### Shuffle the node features & Change the number of random shuffling iterations we want to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db0caaf-05b2-4164-b33c-624683e4a744",
   "metadata": {},
   "outputs": [],
   "source": [
    "aucs = [] # Empty list to store the AUC values\n",
    "feature_dict = {} # Feature dictionary to store the feature change values\n",
    "flag_found = False # Flag to check if Attacking Team feature is included in training\n",
    "iterations = 10 # Number of iterations\n",
    "\n",
    "# Check if Attacking Team Flag exists\n",
    "for flag_index, feature in enumerate(node_features):\n",
    "    if feature == 'Attacking Team Flag':\n",
    "        flag_found = True\n",
    "        break\n",
    "\n",
    "# If Attacking Team Flag does not exist - feature importance can't be performed.\n",
    "if not flag_found:\n",
    "    print(\"Attacking team Flag not included in node features. Can't compute feature importance.\")\n",
    "    \n",
    "else:\n",
    "    for i in range(len(node_features)): \n",
    "        mini_auc = []\n",
    "        for _ in range(iterations):\n",
    "            # Get shuffled data\n",
    "            shuffle_data = ShuffledCounterDataset(data = data,\n",
    "                                          node_feature_shuffle = i, player_type = player_type.value.lower(), \n",
    "                                          matrix_type = adj_matrix.value, flag_index = flag_index)\n",
    "            \n",
    "            # Split between training and testing data\n",
    "            idxs = np.random.RandomState(seed=35).permutation(len(shuffle_data))\n",
    "            split_va, split_te = int(0.7 * len(dataset)), int(0.7 * len(shuffle_data))\n",
    "            idx_tr, idx_va, idx_te = np.split(idxs, [split_va, split_te])\n",
    "            dataset_tr = shuffle_data[idx_tr]\n",
    "            dataset_va = shuffle_data[idx_va]\n",
    "            dataset_te = shuffle_data[idx_te]\n",
    "            loader_te = DisjointLoader(dataset_te, batch_size=16, epochs=1)\n",
    "\n",
    "            # Obtain the model metrics\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "\n",
    "            for batch in loader_te:\n",
    "                inputs, target = batch\n",
    "                p = model(inputs, training=False)\n",
    "                y_true.append(target)\n",
    "                y_pred.append(p.numpy())\n",
    "\n",
    "            y_true = np.vstack(y_true)\n",
    "            y_pred = np.vstack(y_pred)\n",
    "            fpr, tpr, threshold = metrics.roc_curve(y_true, y_pred)\n",
    "            roc_auc = metrics.auc(fpr, tpr)\n",
    "            # Store the AUC's at all iterations\n",
    "            mini_auc.append(roc_auc)\n",
    "\n",
    "        # Perform the error calculation and store it in the aucs list.\n",
    "        errors = [1 - auc_1 for auc_1 in mini_auc]\n",
    "        main_error = 1 - roc_auc\n",
    "        errors_ = [100*(error - main_error) for error in errors]\n",
    "        feature_dict[node_features[i]] = errors_\n",
    "        aucs.append(sum(mini_auc) / len(mini_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eee694-0802-4d60-ab96-9f309aa0ce5e",
   "metadata": {},
   "source": [
    "#### Box plot to inspect feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ad4488-2cd7-4d98-8ac2-1fa56792a668",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(10,25))\n",
    "\n",
    "box_plot = plt.boxplot(feature_dict.values(),\n",
    "                               positions=np.array(\n",
    "    np.arange(len(feature_dict.keys())))*2.0+0.3,\n",
    "                               widths=0.6, vert = False)\n",
    "    \n",
    "    \n",
    "def define_box_properties(plot_name, color_code = 'black', label = ''):\n",
    "    '''\n",
    "    Define Box plot properties.\n",
    "    '''\n",
    "    for k, v in plot_name.items():\n",
    "        plt.setp(plot_name.get(k), color=color_code)\n",
    "         \n",
    "    # use plot function to draw a small line to name the legend.\n",
    "    plt.plot([], c=color_code, label=label)\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "define_box_properties(box_plot)\n",
    "\n",
    "plt.yticks(np.arange(0, len(feature_dict.keys()) * 2, 2), feature_dict.keys())\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.title('Feature Importance - Box Plot')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
